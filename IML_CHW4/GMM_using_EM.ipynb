{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\">An Introduction to Machine Learning - 25737</h1>\n",
        "<h4 align=\"center\">Dr. Yasaei</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Autumn 2024</h4>\n",
        "\n",
        "**Student Name**: Adel Movahedian\n",
        "\n",
        "**Student ID**: 400102074"
      ],
      "metadata": {
        "id": "UUVoHJM1Xed5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Mixture Models with EM\n",
        "\n",
        "## Introduction and Purpose\n",
        "\n",
        "In this exercise, you will:\n",
        "\n",
        "1. Implement a **Gaussian Mixture Model (GMM)** using the Expectation-Maximization (EM) algorithm **from scratch** (using NumPy and basic Python operations).\n",
        "2. Implement the **same GMM model using PyTorch**.\n",
        "3. Compare and contrast the two implementations (performance, complexity, ease of coding, etc.).\n",
        "\n",
        "**Gaussian Mixture Models** assume that data is generated from a mixture of several Gaussian distributions. The EM algorithm iteratively updates the parameters (means, covariances, and mixture weights) of these Gaussians to maximize the likelihood of observed data.\n",
        "\n"
      ],
      "metadata": {
        "id": "_q3lWTPWu_Wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data Loading and Exploration\n",
        "\n",
        "**Tasks:**  \n",
        "- Load the Iris dataset and store the features in `X` and labels in `y`.\n",
        "- Print the shape of `X` and examine a few rows.\n",
        "- **Hint:** Use `sklearn.datasets.load_iris()` to load the data."
      ],
      "metadata": {
        "id": "R7YUHxtMvPOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yTjogaS8u-Sf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a82a0e-9fd4-4ad6-9443-084677be5d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (150, 4)\n",
            "First 5 samples:\n",
            " [[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load the Iris dataset and print shape\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"First 5 samples:\\n\", X[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data Preprocessing (Scaling)\n",
        "\n",
        "**Tasks:**  \n",
        "- Scale the data using `StandardScaler` so that each feature has zero mean and unit variance.\n",
        "- **Hint:** `from sklearn.preprocessing import StandardScaler`.\n"
      ],
      "metadata": {
        "id": "5MxjH7L4vVVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Scale the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Mean after scaling:\", X_scaled.mean(axis=0))\n",
        "print(\"Std after scaling:\", X_scaled.std(axis=0))\n"
      ],
      "metadata": {
        "id": "kIxnM1ybvSpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529f7aec-ccf9-498a-97a9-616dbe479657"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean after scaling: [-1.69031455e-15 -1.84297022e-15 -1.69864123e-15 -1.40924309e-15]\n",
            "Std after scaling: [1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Implementing GMM with EM **from scratch** (NumPy-based)\n",
        "\n",
        "We will first implement GMM using NumPy arrays and basic operations, without PyTorch.\n",
        "\n",
        "**Tasks:**  \n",
        "- Choose the number of components `K` (e.g., K=3).\n",
        "- Initialize the parameters: means, covariances (diagonal), and mixture weights.\n",
        "- Write functions for the E-step and M-step of the EM algorithm.\n",
        "- Run the EM algorithm for a fixed number of iterations.\n",
        "\n",
        "**Hints for Implementation:**\n",
        "\n",
        "- Means: K x D array.\n",
        "- Covariances: K x D x D (diagonal only, so you mainly store variances per feature).\n",
        "- Weights: K-dimensional array, summing to 1.\n",
        "- To compute Gaussian densities, recall the formula for the probability density of a multivariate Gaussian.\n",
        "- For the E-step, compute responsibilities using the mixture components and their densities.\n",
        "- For the M-step, update means, covariances, and weights using the responsibilities.\n",
        "\n",
        "After implementing and running EM, extract cluster assignments by taking `argmax` of responsibilities.\n"
      ],
      "metadata": {
        "id": "QlrQIMl7vYde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set number of components\n",
        "K = 3\n",
        "N, D = X_scaled.shape\n",
        "\n",
        "# TODO: Initialize means, covariances, and weights\n",
        "np.random.seed(42) #for reproducibility\n",
        "means = np.random.rand(K, D)\n",
        "covariances = np.ones((K, D))  # Diagonal covariances, so you can store just var per component and construct diag\n",
        "weights = np.ones(K) / K\n",
        "\n",
        "# TODO: Define a function to compute Gaussian PDF values for each component\n",
        "def gaussian_pdf(X, mean, cov):\n",
        "    # X: N x D\n",
        "    # mean: D\n",
        "    # cov: D x D (diagonal)\n",
        "    diff = X - mean\n",
        "    exponent = -0.5 * np.sum((diff**2) / cov, axis=1)\n",
        "    normalization = np.sqrt((2 * np.pi)**D * np.prod(cov))\n",
        "    return np.exp(exponent) / normalization\n",
        "\n",
        "# TODO: E-step\n",
        "def e_step(X, means, covariances, weights):\n",
        "    responsibilities = np.zeros((N, K))\n",
        "    for k in range(K):\n",
        "        pdf = gaussian_pdf(X, means[k], covariances[k])\n",
        "        responsibilities[:, k] = weights[k] * pdf\n",
        "    responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
        "    return responsibilities\n",
        "\n",
        "# TODO: M-step\n",
        "def m_step(X, responsibilities):\n",
        "    N_k = responsibilities.sum(axis=0)\n",
        "    means = (responsibilities.T @ X) / N_k[:, np.newaxis]\n",
        "    covariances = np.zeros((K, D))\n",
        "    for k in range(K):\n",
        "        diff = X - means[k]\n",
        "        covariances[k] = (responsibilities[:, k][:, np.newaxis] * diff**2).sum(axis=0) / N_k[k]\n",
        "    weights = N_k / N\n",
        "    return means, covariances, weights\n",
        "\n",
        "# Run EM\n",
        "max_iter = 100\n",
        "tol = 1e-6\n",
        "log_likelihoods = []\n",
        "for iteration in range(max_iter):\n",
        "    responsibilities = e_step(X_scaled, means, covariances, weights)\n",
        "    new_means, new_covariances, new_weights = m_step(X_scaled, responsibilities)\n",
        "    log_likelihood = np.sum(\n",
        "        np.log(\n",
        "            np.sum(\n",
        "                [weights[k] * gaussian_pdf(X_scaled, new_means[k], new_covariances[k]) for k in range(K)],\n",
        "                axis=0,\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    log_likelihoods.append(log_likelihood)\n",
        "    if iteration > 0 and abs(log_likelihood - log_likelihoods[-2]) < tol:\n",
        "        print(f\"Converged at iteration {iteration}\")\n",
        "        break\n",
        "    means, covariances, weights = new_means, new_covariances, new_weights\n",
        "cluster_labels_numpy = np.argmax(responsibilities, axis=1) # argmax of responsibilities\n",
        "# for my visulization\n",
        "print(\"Final means:\\n\", means)\n",
        "print(\"Final weights:\\n\", weights)\n",
        "print(\"Cluster labels:\\n\", cluster_labels_numpy)"
      ],
      "metadata": {
        "id": "EclPwJbfvah5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b480c74a-4987-4950-96be-9da3f4310ff6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at iteration 36\n",
            "Final means:\n",
            " [[ 1.17071161  0.03193007  1.11770039  1.19342422]\n",
            " [-1.01457897  0.85326268 -1.30498732 -1.25489349]\n",
            " [ 0.10225927 -0.70659069  0.36847724  0.28190811]]\n",
            "Final weights:\n",
            " [0.25272081 0.33333333 0.41394586]\n",
            "Cluster labels:\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0\n",
            " 0 0 2 0 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 2 0 0 0 2 0 0 0 2 0 0 0 2 0\n",
            " 0 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Implementing GMM with EM **using PyTorch**\n",
        "\n",
        "Now, we will implement the same algorithm using PyTorch tensors. The steps are similar, but you will use `torch` operations. This might simplify certain operations and open the door to GPU acceleration.\n",
        "\n",
        "**Tasks:**  \n",
        "- Convert `X_scaled` to a PyTorch tensor.\n",
        "- Initialize parameters as `torch.tensor`s.\n",
        "- Implement E-step and M-step in PyTorch.\n",
        "- Run EM for a fixed number of iterations.\n",
        "- Extract cluster labels.\n",
        "\n",
        "**Hints:**\n",
        "- Use `torch.tensor(X_scaled, dtype=torch.float32)` to create a PyTorch tensor.\n",
        "- Operations are similar but use `torch.sum`, `torch.exp`, etc.\n",
        "- Watch out for broadcasting rules and ensure shapes align.\n"
      ],
      "metadata": {
        "id": "deQSDjycvmIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# TODO: Convert data to torch tensor\n",
        "X_torch = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "K = 3\n",
        "N, D = X_torch.shape\n",
        "\n",
        "# TODO: Initialize means, covariances, weights as torch tensors\n",
        "torch.manual_seed(42)\n",
        "means_torch = torch.rand((K, D), dtype=torch.float32)\n",
        "covariances_torch = torch.ones((K, D), dtype=torch.float32)\n",
        "weights_torch = torch.ones(K, dtype=torch.float32) / K\n",
        "\n",
        "# TODO: Implement gaussian_pdf using torch operations\n",
        "def gaussian_pdf_torch(X, mean, cov):\n",
        "    diff = X - mean\n",
        "    exponent = -0.5 * torch.sum((diff**2) / cov, dim=1)\n",
        "    normalization = torch.sqrt((2 * torch.pi)**D * torch.prod(cov))\n",
        "    return torch.exp(exponent) / normalization\n",
        "\n",
        "# TODO: E-step in torch\n",
        "def e_step_torch(X, means, covariances, weights):\n",
        "    responsibilities = torch.zeros(N, K, dtype=torch.float32)\n",
        "    for k in range(K):\n",
        "        pdf = gaussian_pdf_torch(X, means[k], covariances[k])\n",
        "        responsibilities[:, k] = weights[k] * pdf\n",
        "    responsibilities /= responsibilities.sum(dim=1, keepdim=True)\n",
        "    return responsibilities\n",
        "\n",
        "# TODO: M-step in torch\n",
        "def m_step_torch(X, responsibilities):\n",
        "    N_k = responsibilities.sum(dim=0)\n",
        "    means = (responsibilities.T @ X) / N_k[:, None]\n",
        "    covariances = torch.zeros((K, D), dtype=torch.float32)\n",
        "    for k in range(K):\n",
        "        diff = X - means[k]\n",
        "        covariances[k] = (responsibilities[:, k][:, None] * diff**2).sum(dim=0) / N_k[k]\n",
        "    weights = N_k / N\n",
        "    return means, covariances, weights\n",
        "\n",
        "# Run EM in torch\n",
        "max_iter = 100\n",
        "tol = 1e-6\n",
        "log_likelihoods_torch = []\n",
        "\n",
        "for iteration in range(max_iter):\n",
        "    responsibilities_torch = e_step_torch(X_torch, means_torch, covariances_torch, weights_torch)\n",
        "    new_means_torch, new_covariances_torch, new_weights_torch = m_step_torch(X_torch, responsibilities_torch)\n",
        "    log_likelihood_torch = torch.sum(\n",
        "        torch.log(\n",
        "            torch.sum(\n",
        "                torch.stack([\n",
        "                    weights_torch[k] * gaussian_pdf_torch(X_torch, new_means_torch[k], new_covariances_torch[k])\n",
        "                    for k in range(K)\n",
        "                ], dim=0),\n",
        "                dim=0,\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    log_likelihoods_torch.append(log_likelihood_torch.item())\n",
        "    if iteration > 0 and abs(log_likelihood_torch - log_likelihoods_torch[-2]) < tol:\n",
        "        print(f\"Converged at iteration {iteration}\")\n",
        "        break\n",
        "    means_torch, covariances_torch, weights_torch = new_means_torch, new_covariances_torch, new_weights_torch\n",
        "cluster_labels_torch = torch.argmax(responsibilities_torch, dim=1)\n",
        "# for my visulization\n",
        "print(\"Final means (PyTorch):\\n\", means_torch)\n",
        "print(\"Final weights (PyTorch):\\n\", weights_torch)\n",
        "print(\"Cluster labels (PyTorch):\\n\", cluster_labels_torch)\n"
      ],
      "metadata": {
        "id": "_-G1vfhGvi4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20aeedbe-981f-42aa-d8f2-f72a42cce335"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at iteration 22\n",
            "Final means (PyTorch):\n",
            " tensor([[ 1.1719,  0.0327,  1.1183,  1.1940],\n",
            "        [-1.0146,  0.8533, -1.3050, -1.2549],\n",
            "        [ 0.1025, -0.7064,  0.3688,  0.2824]])\n",
            "Final weights (PyTorch):\n",
            " tensor([0.2523, 0.3333, 0.4143])\n",
            "Cluster labels (PyTorch):\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2,\n",
            "        0, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0,\n",
            "        0, 0, 2, 0, 0, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Evaluating and Comparing Both Implementations\n",
        "\n",
        "**Tasks:**  \n",
        "- Use `adjusted_rand_score` to compare the cluster labels from both methods against the true labels `y`.\n",
        "- Print the ARI for both NumPy and PyTorch implementations.\n",
        "- Visually inspect if both implementations yield similar results.\n",
        "\n",
        "**Questions:**\n",
        "- Are the ARI scores similar or different between the two implementations?\n",
        "- Which code was easier to write and maintain?\n",
        "- Which implementation might be easier to extend to more complex models?\n"
      ],
      "metadata": {
        "id": "EcEETR0Hvsh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "ari_numpy = adjusted_rand_score(y, cluster_labels_numpy)\n",
        "print(\"ARI (NumPy):\", ari_numpy)\n",
        "ari_torch = adjusted_rand_score(y, cluster_labels_torch.numpy())\n",
        "print(\"ARI (PyTorch):\", ari_torch)\n",
        "print(\"\\nComparison:\")\n",
        "print(\"Are the ARI scores similar or different?\")\n",
        "if abs(ari_numpy - ari_torch) < 1e-3:\n",
        "    print(\"The ARI scores are very similar.\")\n",
        "else:\n",
        "    print(\"The ARI scores are different.\")\n",
        "print(\"\\nWhich code was easier to write and maintain?\")\n",
        "print(\"The NumPy implementation was easier to write initially, but the PyTorch implementation might be easier to extend and maintain due to its support for GPU acceleration and compatibility with modern deep learning workflows.\")\n",
        "print(\"\\nWhich implementation might be easier to extend to more complex models?\")\n",
        "print(\"The PyTorch implementation would be easier to extend to more complex models due to its modularity, GPU support, and integration with other PyTorch functionalities.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xmumvOglvuJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6329bf3b-7620-4037-d575-136ca1a86d70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARI (NumPy): 0.7591987071071522\n",
            "ARI (PyTorch): 0.7591987071071522\n",
            "\n",
            "Comparison:\n",
            "Are the ARI scores similar or different?\n",
            "The ARI scores are very similar.\n",
            "\n",
            "Which code was easier to write and maintain?\n",
            "The NumPy implementation was easier to write initially, but the PyTorch implementation might be easier to extend and maintain due to its support for GPU acceleration and compatibility with modern deep learning workflows.\n",
            "\n",
            "Which implementation might be easier to extend to more complex models?\n",
            "The PyTorch implementation would be easier to extend to more complex models due to its modularity, GPU support, and integration with other PyTorch functionalities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**  \n",
        "1. **Implementation Detail:** What are the main differences in code complexity between a plain NumPy-based implementation and a PyTorch-based one?  \n",
        "   <font color='green'>   \n",
        "   The NumPy implementation requires manually handling broadcasting, numerical stability, and managing operations at a low level. In contrast, the PyTorch implementation abstracts many of these details, providing a cleaner syntax with built-in functions for operations like matrix multiplication, broadcasting, and automatic differentiation. However, PyTorch requires a bit more setup, such as converting data to tensors, which adds slight overhead in simplicity for new users.\n",
        "   </font>\n",
        "<br>\n",
        "2. **Performance:** Which implementation is likely to be more efficient or easier to parallelize and why?  \n",
        "   <font color='green'>  \n",
        "   The PyTorch implementation is likely to be more efficient and easier to parallelize because PyTorch is designed for high-performance computations, including automatic use of GPUs and multi-threaded CPU operations. Its tensor operations are optimized for parallel execution, whereas NumPy is primarily CPU-based and less optimized for parallelism in comparison.\n",
        "   </font>\n",
        "<br>\n",
        "\n",
        "3. **Numerical Stability:** How might PyTorch’s built-in functions improve numerical stability compared to a manual implementation?  \n",
        "   <font color='green'>  \n",
        "   PyTorch's built-in functions are optimized for numerical stability. For instance, operations like `torch.logsumexp` prevent issues with underflow/overflow in logarithmic computations. Additionally, PyTorch leverages highly optimized libraries (e.g., cuDNN, BLAS) to handle edge cases and maintain precision, reducing the likelihood of numerical errors compared to manual implementations in NumPy.\n",
        "   </font>\n",
        "<br>\n",
        "4. **Extendability:** If you wanted to add more complex features (e.g., full covariance matrices, regularization), which approach would be simpler and why?  \n",
        "   <font color='green'>  \n",
        "   The PyTorch approach would be simpler to extend because of its modular and object-oriented nature. Complex features like full covariance matrices can be implemented using existing tensor operations, and PyTorch's autograd system would handle derivatives for optimization seamlessly. In contrast, adding such features in NumPy would require manually managing the additional complexity, making the implementation prone to errors and harder to maintain.\n",
        "   </font>\n",
        "<br>"
      ],
      "metadata": {
        "id": "TMzyGBV1wP4D"
      }
    }
  ]
}