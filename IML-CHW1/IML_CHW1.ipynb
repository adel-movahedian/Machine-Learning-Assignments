{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0Y4B_4b3lnJ"
      },
      "source": [
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
        "<font color=0F5298 size=7>\n",
        "Introduction to Machine Learning - 25732 <br>\n",
        "<font color=2565AE size=5>\n",
        "Department of Electrical Enginnering<br>\n",
        "Dr. Mohammad Hossein Yassaee<br>\n",
        "Fall 2024<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSltFU5u7nLs"
      },
      "source": [
        "### Information:\n",
        "\n",
        "1.   Full Name:adel movahedian\n",
        "2.   Student Number: 400102074"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wtyv2jR3lnL"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "*I. You are just allowded to change those parts that start with \"TO DO\". Please do not change other parts.*\n",
        "\n",
        "*II. It is highly recommended to read each codeline carefully and try to understand what it exactly does.*\n",
        "\n",
        "*III. Do not copy codes completely from internet sources such as Chat-GPT or etc. If you are using any sources, please put its link in the beging of the block.*\n",
        "\n",
        "*IV. Question 6 has additional points.*\n",
        "\n",
        "If you have any question you can contact related TAs:\n",
        " - Homeworks coordinator: @danialayati\n",
        " - Questions 1, 2, 3 & 6: @armin_dh\n",
        " - Question 4 & 5: @Rosebaekfany\n",
        "\n",
        "\n",
        " Best of luck and have fun!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYy0iYLy3lnL"
      },
      "source": [
        "<font color=3C99D size=5>\n",
        "Question One: Numpy Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26PX8_4T3lnM"
      },
      "source": [
        "##  Part I: Image Processing with NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONYYSp0G3lnM"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHdEmfc53lnM"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import cv2\n",
        "# for google colab\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymqqc0FA3lnN"
      },
      "source": [
        "In this section, First you should load an image and convert it to a gray scaled image then we'll process it by some operations using numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "id": "lCoiwAm03lnN",
        "outputId": "80be46ed-c853-4a84-9104-ee25ccc36799"
      },
      "outputs": [],
      "source": [
        "# read and show the image\n",
        "image = cv2.imread('download.jpeg')\n",
        "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "cv2_imshow(gray_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8lm0Hsb3lnO"
      },
      "source": [
        "<b> From now on, you must work with the grayscale image in each cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZNhuqQf3lnO"
      },
      "source": [
        "In gray scaled 0 means black and 255 is white and each number between these two shows a color in range of black to white. Now for this task, Make pixels brighter than 150, 100 degree darker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "id": "bIh6gn3L3lnO",
        "outputId": "cb901726-d8bc-4265-af3b-4be7d6b24f6c"
      },
      "outputs": [],
      "source": [
        "# TODO: don't use loop or anything like that and try to code by numpy where\n",
        "darker_image = np.where(gray_image > 150, np.clip(gray_image - 100, 0, 255), gray_image)\n",
        "cv2_imshow(darker_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k4rmfns3lnO"
      },
      "source": [
        "In grayscale images, pixel values typically range from 0 to 255, where 0 represents black and 255 represents white. Subtracting each pixel value from 255 effectively produces the negative of the grayscale image. Now we want to create negative grayscale version of our image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "id": "AtC3PHc03lnO",
        "outputId": "34fcc68d-e20c-4641-cdff-111c551504a5"
      },
      "outputs": [],
      "source": [
        "# TODO: Create negative grayscale image\n",
        "negative_grayscale_image = 255 - gray_image\n",
        "cv2_imshow(negative_grayscale_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tCGkRx0p3lnP",
        "outputId": "1c13f5cb-210a-42fb-a142-c6190cc55b79"
      },
      "outputs": [],
      "source": [
        "# TODO: Split the image horizontally into two equal parts using NumPy's split function.\n",
        "height, width = negative_grayscale_image.shape\n",
        "mid = height // 2\n",
        "upper_half, lower_half = np.split(negative_grayscale_image, [height // 2])\n",
        "cv2_imshow(upper_half)\n",
        "print(\".........................................................\")\n",
        "cv2_imshow(lower_half)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Oc-JLtY33lnP",
        "outputId": "4b898a7a-9418-43b4-b808-aafa857749b8"
      },
      "outputs": [],
      "source": [
        "# TODO: Split the image vertically into two equal parts using indexes and slicing.\n",
        "left_half, right_half = negative_grayscale_image[:, :width // 2], negative_grayscale_image[:, width // 2:]\n",
        "cv2_imshow(left_half)\n",
        "cv2_imshow(right_half)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzSFKZAL3lnP"
      },
      "source": [
        "Now We need 4 vertical split (vsplit) from image. write the code in a way that can be used for each image (not hard code size)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z6FJLcaR3lnP",
        "outputId": "1b632190-a056-4e42-bb44-41c2746013b5"
      },
      "outputs": [],
      "source": [
        "# TODO: Split the image vertically into four equal parts using numpy vsplit function.\n",
        "part1, part2, part3, part4 = negative_grayscale_image[:, :width // 4], negative_grayscale_image[:, width // 4:width // 2], negative_grayscale_image[:, width // 2:width * 3 // 4], negative_grayscale_image[:, width * 3 // 4:]\n",
        "cv2_imshow(part1)\n",
        "cv2_imshow(part2)\n",
        "cv2_imshow(part3)\n",
        "cv2_imshow(part4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEhMl9W23lnP"
      },
      "source": [
        "In this section first split the original image horizontally calling them left and right images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "3UxsP8Bb3lnP"
      },
      "outputs": [],
      "source": [
        "#TODO split image\n",
        "left,right = image[:, :width // 2], image[:, width // 2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6vhJk6te3lnP",
        "outputId": "3b44f95a-05c0-4000-b4dd-c1ecccb7990a"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(left)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PD0T9s8e3lnQ",
        "outputId": "c1be283a-e4be-4139-c694-19c6809cd856"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(right)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_01APn2t3lnQ"
      },
      "source": [
        "Now, using  invert the left image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c3eaZUKF3lnQ",
        "outputId": "2abe0e41-d878-4fe1-e029-cf5d53f1109e"
      },
      "outputs": [],
      "source": [
        "#TODO: invert left\n",
        "inverted_left = 255 - left\n",
        "\n",
        "cv2_imshow(inverted_left)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f5ewtnA3lnQ"
      },
      "source": [
        "Now, mirror both the inverted left image and the right image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v4Oe3SXM3lnQ",
        "outputId": "571f9845-fcea-48ec-fb84-d0bdad230b02"
      },
      "outputs": [],
      "source": [
        "#TODO: flip inverted left image\n",
        "mirrored_left = cv2.flip(inverted_left, 0)\n",
        "\n",
        "cv2_imshow(mirrored_left)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cLQ0B0w_3lnQ",
        "outputId": "54098321-0a7f-4f13-ca4d-4317ecd539f1"
      },
      "outputs": [],
      "source": [
        "#TODO: flip right image\n",
        "mirrored_right = cv2.flip(right, 0)\n",
        "\n",
        "cv2_imshow(mirrored_right)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XShXpKt73lnQ"
      },
      "source": [
        "In the next step we want to concat the two images horizontally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "id": "KjxHmRsc3lnQ",
        "outputId": "197de43d-19e4-412d-97bb-456441082290"
      },
      "outputs": [],
      "source": [
        "#TODO: concat mirrored_left and mirrored_right\n",
        "final_image = cv2.hconcat([mirrored_left, mirrored_right])\n",
        "\n",
        "cv2_imshow(final_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfBJw1r43lnR"
      },
      "source": [
        "For the final step invert the concated image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "id": "yjgQltKv3lnR",
        "outputId": "940fecd2-8e94-493b-d19b-a8d8b49efd16"
      },
      "outputs": [],
      "source": [
        "#TODO: invert final_image\n",
        "invert_final_image = 255 - final_image\n",
        "\n",
        "cv2_imshow(invert_final_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc8QERBO3lnR"
      },
      "source": [
        "##  Part II: Broadcasting with NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkJ-UMJ33lnR"
      },
      "source": [
        "In this question, we have several vectors. From them, we want to find a vector that is most similar to a specific vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU6F4-xa3lnR"
      },
      "source": [
        "In order to compare how \"similar\" two vectors are, we define the D parameter like below (a and b are the two vectors we want to compare, with n indices). the smaller the value of D for 2 vectors is, the more similar those two vectors are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmI_ddBJ3lnR"
      },
      "source": [
        "$D=\\sqrt{\\Sigma_{i=1}^n(a_i-b_i)^2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5ecIsrF3lnR"
      },
      "source": [
        "Inputs: in the first line the users gives the value m, which is the number of vectors. In each of the next m lines, the user will give a vector as the input. In the next line, the users gives the vector v."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WDb4TcH3lnR"
      },
      "source": [
        "You are expected to find the closest vector to v."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezW6LD6Y3lnS"
      },
      "outputs": [],
      "source": [
        "#in this cell, you should only get the inputs\n",
        "\n",
        "#TODO: get the number m\n",
        "m = int(input(\"Enter the number of vectors (m): \"))\n",
        "#TODO: get m vectors\n",
        "vectors = []\n",
        "for i in range(m):\n",
        "    vector_input = input(f\"Enter vector {i+1} (space-separated): \")\n",
        "    vector = list(map(float, vector_input.split()))\n",
        "    vectors.append(vector)\n",
        "#TODO: get the vector v\n",
        "v_input = input(\"Enter the vector v to compare (space-separated): \")\n",
        "v = list(map(float, v_input.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVvKUmEm3lnS"
      },
      "outputs": [],
      "source": [
        "#in this cell you should find the expected output. you are not allowed to use loops in this cell.\n",
        "vectors_array = np.array(vectors)\n",
        "v_array = np.array(v)\n",
        "#TODO: find the difference of all vectors with v\n",
        "differences = vectors_array - v_array\n",
        "#TODO: compute D for each vector\n",
        "D_values = np.sqrt(np.sum(differences**2, axis=1))\n",
        "\n",
        "#TODO: find the vector with the lowest D\n",
        "closest_vector_index = np.argmin(D_values)\n",
        "#TODO: print the most similar vector\n",
        "most_similar_vector = vectors_array[closest_vector_index]\n",
        "\n",
        "print(\"Most similar vector:\", most_similar_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgMC0PgC3lnT"
      },
      "source": [
        "<font color=3C99D size=5>\n",
        "Question Two: Introduction to Pandas\n",
        "\n",
        "In this part you will learn how to work with Pandas library and how to convert raw data into suitable data for model input ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqMpK50F3lnU"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LKMdqzJ3lnU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from matplotlib import rc\n",
        "import matplotlib.ticker as ticker\n",
        "from math import sqrt\n",
        "%matplotlib inline\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTJJ4oGt3lnU"
      },
      "outputs": [],
      "source": [
        "# Change the address as you wish\n",
        "plays = pd.read_csv('user_artists.dat', sep='\\t')\n",
        "artists = pd.read_csv('artists.dat', sep='\\t', usecols=['id','name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2YwCvVO3lnU"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfIw9hPn3lnU"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "## TODO: merge two DataSets in a way that below DataFrame is created         ##\n",
        "###############################################################################\n",
        "\n",
        "ap = pd.merge(plays, artists, left_on='artistID', right_on='id')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rSkRTCj3lnU"
      },
      "outputs": [],
      "source": [
        "ap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiEc2zCB3lnV"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "## TODO: rename the weight columns to PlayCount         ##\n",
        "###############################################################################\n",
        "ap = ap.rename(columns={\"weight\": \"playCount\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rUff2zl3lnV"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "## TODO: Delete the \"id\" columns and reset_indexes of ap                     ##\n",
        "###############################################################################\n",
        "\n",
        "#TODO\n",
        "ap = ap.drop(columns=['id'])\n",
        "\n",
        "ap = ap.reset_index(drop=True)\n",
        "\n",
        "print(ap.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0YXP9xW3lnV"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "## TODO: Create  a Dataframe like below where \"totalUniqueUsers\" is the number of\n",
        "## userID's assigned to each name in \"ap\" dataframe.\n",
        "## \"totalArtistPlays\" is sum of all playcounts assigned to an artist\n",
        "## and sort the Frame in descending order of \"totalArtistPlays\" (highest \"totalArtistPlays\" at top of table)\n",
        "## and \"avgUserPlays\" is divison of \"totalArtistPlays\" by \"totalUniqueUsers\"\n",
        "###############################################################################\n",
        "\n",
        "artist_rank = ap.groupby('name').agg(\n",
        "    totalUniqueUsers=pd.NamedAgg(column='userID', aggfunc=pd.Series.nunique),\n",
        "    totalArtistPlays=pd.NamedAgg(column='playCount', aggfunc='sum')\n",
        ").reset_index()\n",
        "\n",
        "artist_rank['avgUserPlays'] = artist_rank['totalArtistPlays'] / artist_rank['totalUniqueUsers']\n",
        "\n",
        "artist_rank = artist_rank.sort_values(by='totalArtistPlays', ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q72s7HoG3lnV"
      },
      "outputs": [],
      "source": [
        "artist_rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3Fgx-gV3lnV"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "## TODO: Create  a Dataframe like below where for each name in ap dataframe,\n",
        "# corresponding row in artist rank is attached to it\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "\n",
        "ap = ap.merge(artist_rank, on='name', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOIQAZik3lnV"
      },
      "outputs": [],
      "source": [
        "ap.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkIan_kn3lnV"
      },
      "source": [
        "# Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VT91Yww3lnW"
      },
      "outputs": [],
      "source": [
        "def bar_chart_int(x, y, x_label, y_label, title, caption, total_val):\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_size_inches(16, 5)\n",
        "    ax = sns.barplot(x=x[:20], y=y[:20], palette='Blues_r')  # Use x= and y=\n",
        "    ax.set_xlabel(x_label)\n",
        "    ax.set_ylabel(y_label)\n",
        "    ax.set_title(title)\n",
        "    ax.get_yaxis().set_major_formatter(ticker.FuncFormatter(lambda x, p: '{:,}'.format(int(x))))\n",
        "\n",
        "    # Bar label placement\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        pct = 100 * (height / total_val)\n",
        "        ax.text(p.get_x() + p.get_width() / 2.,\n",
        "                height + 3,\n",
        "                '{:1.1f}%'.format(pct),\n",
        "                ha=\"center\", verticalalignment='bottom', color='black', fontsize=12)\n",
        "\n",
        "    # our caption statement\n",
        "    ax.text(19, max(y[:20]) * 0.95, caption, horizontalalignment='right')\n",
        "\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyY3Eg403lnW"
      },
      "outputs": [],
      "source": [
        "c1 = artist_rank\n",
        "x = c1.index\n",
        "y = c1.totalArtistPlays\n",
        "x_label = 'Artist Name'\n",
        "y_label = 'Total Artist Plays'\n",
        "title = 'Total Plays by Artist'\n",
        "caption = 'Percentage of total plays'\n",
        "total_val = c1.totalArtistPlays.sum()\n",
        "\n",
        "bar_chart_int(x, y, x_label, y_label, title, caption, total_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lPFiFrt3lnW"
      },
      "outputs": [],
      "source": [
        "c2 = artist_rank.sort_values(['totalUniqueUsers'],ascending=False)\n",
        "x = c2.index\n",
        "y = c2.totalUniqueUsers\n",
        "x_label = 'Artist Name'\n",
        "y_label = 'Unique Users Played'\n",
        "title = 'Unique Users per Artist'\n",
        "caption = 'Percentage of total unique users'\n",
        "total_val = ap.userID.nunique()\n",
        "\n",
        "bar_chart_int(x,y,x_label,y_label,title,caption,total_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNVWG9yO3lnW"
      },
      "outputs": [],
      "source": [
        "top_artists = top_artists = artist_rank.nlargest(12, 'totalArtistPlays').index  # TODO : index of top 12 items in \"artist_rank\" based on their \"totalArtistPlays\"\n",
        "\n",
        "x = artist_rank['totalUniqueUsers']# TODO : \"totalUniqueUsers\" of \"artist_rank\"\n",
        "y = artist_rank['totalArtistPlays'] # TODO :  \"totalArtistPlays\" of \"artist_rank\"\n",
        "labels = artist_rank['name']  # TODO : index of \"artist_rank\"\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "sns.regplot(x=x, y=y, ax=ax)\n",
        "ax.set_title('Artist Popularity: Play Count vs Unique Users')\n",
        "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:,.0f}'.format(y)))\n",
        "ax.set_xlabel('Total Unique Users')\n",
        "ax.set_ylabel('Total Artist Plays')\n",
        "\n",
        "for i, t in enumerate(labels):\n",
        "    if t in top_artists:\n",
        "        ax.annotate(t,(x[i],y[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUND1ds13lnW"
      },
      "outputs": [],
      "source": [
        "pc = ap.playCount\n",
        "play_count_scaled = (pc - pc.min()) / (pc.max() - pc.min())\n",
        "ap = ap.assign(playCountScaled=play_count_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB2KUTNZ3lnW"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "## TODO: Create  a Dataframe with userID as its rows, artistID as its columns and\n",
        "## \"play_count_scaled\" as the value of each cell, in this manner you're creating\n",
        "## a table which demonstrates what rating each user has for a set of items(artists)\n",
        "## Your created table should be like below example\n",
        "###############################################################################\n",
        "\n",
        "ratings_df = ap.pivot_table(index='userID', columns='artistID', values='playCountScaled')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irmykH_v3lnX"
      },
      "outputs": [],
      "source": [
        "ratings_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxPa6T-33lnX"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# TODO: Fill each NaN value with 0 and return a numpy array corresponding modified DataFrame\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "ratings_filled = ratings_df.fillna(0)\n",
        "\n",
        "ratings_array = ratings_filled.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMXg9PaC3lnX"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# TODO : Calculate how sparse your data is (in percent) and print it, meaning\n",
        "# what fraction of the above table is filled with observed values.\n",
        "###############################################################################\n",
        "\n",
        "## Your Code\n",
        "total_cells = ratings_df.size\n",
        "\n",
        "observed_values = ratings_df.count().sum()\n",
        "\n",
        "sparsity = (observed_values / total_cells) * 100\n",
        "\n",
        "print(f\"Sparsity of data: {sparsity:.2f}%\")\n",
        "\n",
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyU1MDgH5YqD"
      },
      "source": [
        "<font color=3C99D size=5>\n",
        "Question Three: Eigenvalues and Eigenvectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4qDfFzecFNs"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gig8KeSQcFNt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import timeit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3VxD3T6cFNt"
      },
      "source": [
        "## Eigenvalues and Eigenvectors\n",
        "Eigenvalues and eigenvectors are fundamental concepts in linear algebra. For a given square matrix A, if there is a non-zero vector v such that Av = λv, then λ is called the eigenvalue and v is the corresponding eigenvector.\n",
        "\n",
        "## Functions for Eigenvalues and Eigenvectors\n",
        "We will define functions to compute eigenvalues and eigenvectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txnHjY_3cFNu"
      },
      "outputs": [],
      "source": [
        "def compute_eigenvalues(A):\n",
        "    \"\"\"\n",
        "    Computes and returns a list of eigenvalues of the input matrix A.\n",
        "\n",
        "    Args:\n",
        "    A (numpy.ndarray): The input matrix for which eigenvalues are computed.\n",
        "\n",
        "    Returns:\n",
        "    eigenvalues (list): A list containing the eigenvalues of matrix A.\n",
        "    \"\"\"\n",
        "    eigenvalues = np.linalg.eigvals(A)\n",
        "    return eigenvalues.tolist()\n",
        "\n",
        "def compute_eigenvector(A, eigenvalue):\n",
        "    \"\"\"\n",
        "    Computes the eigenvector corresponding to the given eigenvalue of matrix A.\n",
        "\n",
        "    Args:\n",
        "    - A (numpy.ndarray): The square matrix.\n",
        "    - eigenvalue (float or complex): The eigenvalue for which the eigenvector is computed.\n",
        "\n",
        "    Returns:\n",
        "    - eigenvector (numpy.ndarray): The eigenvector corresponding to the input eigenvalue.\n",
        "    \"\"\"\n",
        "    I = np.eye(A.shape[0])\n",
        "    matrix = A - eigenvalue * I\n",
        "    _, _, vh = np.linalg.svd(matrix)\n",
        "    eigenvector = vh[-1]  # Take the last row of V^H for the solution\n",
        "    return eigenvector\n",
        "\n",
        "def compute_eigenvectors(A):\n",
        "    \"\"\"\n",
        "    Computes the eigenvectors of the matrix A.\n",
        "\n",
        "    Args:\n",
        "    - A (numpy.ndarray): The square matrix for which eigenvectors are computed.\n",
        "\n",
        "    Returns:\n",
        "    - eigenvectors (list of numpy.ndarray): A list containing the eigenvectors of matrix A.\n",
        "    \"\"\"\n",
        "    _, eigenvectors = np.linalg.eig(A)\n",
        "    return np.array(eigenvectors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9casgDJIcFNu"
      },
      "source": [
        "## Visualization of Vectors\n",
        "A function to visualize vectors in 2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IeKtF25cFNv"
      },
      "outputs": [],
      "source": [
        "def plot_vectors(vectors, colors, labels, title):\n",
        "    plt.figure()\n",
        "    plt.axhline(0, color='gray', lw=0.5)\n",
        "    plt.axvline(0, color='gray', lw=0.5)\n",
        "    plt.grid()\n",
        "    for vector, color, label in zip(vectors, colors, labels):\n",
        "        plt.quiver(0, 0, vector[0], vector[1], angles='xy', scale_units='xy', scale=1, color=color, label=label)\n",
        "    plt.xlim(-1.5, 1.5)\n",
        "    plt.ylim(-1.5, 1.5)\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtQvOd4LcFNv"
      },
      "source": [
        "## Example with a Random Matrix\n",
        "We will generate a random 2D matrix with two independent eigenvectors and visualize them along with a random vector. Then we apply the transformation and plot them again.\n",
        "### no need to change the below cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVt8UfvtcFNw"
      },
      "outputs": [],
      "source": [
        "def generate_valid_matrix():\n",
        "    while True:\n",
        "        A = np.random.rand(2, 2)\n",
        "        eigenvalues = np.linalg.eigvals(A)\n",
        "        if (np.all(np.isreal(eigenvalues)) and\n",
        "            len(set(eigenvalues)) == 2 and\n",
        "            np.all(np.abs(eigenvalues) > 0.33) and\n",
        "            np.all(np.abs(eigenvalues) < 3)):\n",
        "            return A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgICAOgZcFNw"
      },
      "outputs": [],
      "source": [
        "A = generate_valid_matrix()\n",
        "eigenvectors = compute_eigenvectors(A)\n",
        "\n",
        "random_vector = np.random.rand(2)\n",
        "\n",
        "plot_vectors([eigenvectors[:, 0], eigenvectors[:, 1], random_vector],\n",
        "             ['r', 'g', 'b'],\n",
        "             ['Eigenvector 1', 'Eigenvector 2', 'Random Vector'],\n",
        "             'Original Vectors')\n",
        "\n",
        "transformed_vectors = [A @ eigenvectors[:, 0], A @ eigenvectors[:, 1], A @ random_vector]\n",
        "\n",
        "plot_vectors(transformed_vectors,\n",
        "             ['r', 'g', 'b'],\n",
        "             ['Transformed Eigenvector 1', 'Transformed Eigenvector 2', 'Transformed Random Vector'],\n",
        "             'Transformed Vectors')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYMHiP59cFNx"
      },
      "source": [
        "<h3 style='color: red;'>Question</h3>\n",
        "explain this behavior of these vectors in 2D plain\n",
        "\n",
        "<h3 style='color: yellow;'>Your answer</h3>\n",
        "\n",
        "The plot you provided shows three vectors in a 2D plane: two eigenvectors of a randomly generated matrix \\( A \\) and a randomly generated vector. I give an explanation of their behavior and significance in this context:\n",
        "\n",
        "### Explanation of the Vectors\n",
        "\n",
        "1. **Eigenvectors**:\n",
        "   - The **red vector** and the **green vector** represent the eigenvectors of the matrix \\( A \\). Eigenvectors are significant in linear algebra because they indicate directions in which the transformation represented by the matrix acts simply by stretching or compressing, without changing direction.\n",
        "   - In the plot, the eigenvectors are shown as arrows emanating from the origin. The direction of these arrows indicates the direction of each eigenvector.\n",
        "\n",
        "2. **Random Vector**:\n",
        "   - The **blue vector** is a randomly generated vector. Unlike the eigenvectors, this vector does not necessarily have a special significance regarding the matrix \\( A \\). It represents an arbitrary direction in the 2D space.\n",
        "\n",
        "### Behavior in the 2D Plane\n",
        "\n",
        "1. **Orthogonality**:\n",
        "   - If the eigenvalues of the matrix \\( A \\) are distinct (which is the case here since you used a condition to ensure they are), the eigenvectors will be orthogonal to each other. In the plot, if the angle between the red and green vectors is close to 90 degrees, it suggests that they are indeed orthogonal.\n",
        "   - This orthogonality is important because it means the eigenvectors span the entire space, and any vector in the 2D plane can be represented as a linear combination of these two eigenvectors.\n",
        "\n",
        "2. **Transformation**:\n",
        "   - The eigenvectors represent the principal axes of the transformation defined by the matrix \\( A \\). When you apply the matrix \\( A \\) to these eigenvectors, they will stretch or compress along these directions by their corresponding eigenvalues.\n",
        "   - The random vector (blue) may not align with either eigenvector. When transformed by \\( A \\), it will change direction and length, depending on how \\( A \\) interacts with it.\n",
        "\n",
        "### Visual Interpretation\n",
        "\n",
        "- The **legend** indicates which color corresponds to which vector, allowing you to identify them easily.\n",
        "- The grid lines help visualize the angles and magnitudes of the vectors relative to each other and the coordinate axes.\n",
        "- The arrows provide a clear visual indication of the direction and magnitude of each vector, making it easy to see how the eigenvectors are related to the random vector in the context of the matrix transformation.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The behavior of these vectors in the 2D plane illustrates fundamental concepts in linear algebra, including the significance of eigenvectors, the concept of orthogonality, and how matrices can transform vectors in space. The plot effectively demonstrates the interplay between linear transformations and vector directions in a visually intuitive way.\n",
        "\n",
        "## Diagonalization\n",
        "Diagonalization of a matrix involves finding a matrix P and a diagonal matrix D such that A = PDP^(-1). This is possible if A has n linearly independent eigenvectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amDMFE8VcFNx"
      },
      "outputs": [],
      "source": [
        "def diagonalize(A):\n",
        "    \"\"\"\n",
        "    Diagonalizes the square matrix A if possible using eigenvectors.\n",
        "\n",
        "    Args:\n",
        "    - A (numpy.ndarray): The square matrix to be diagonalized.\n",
        "\n",
        "    Returns:\n",
        "    - diagonal_matrix (numpy.ndarray): The diagonal matrix D such that A = PDP^-1,\n",
        "      where D is diagonal and P is the matrix of eigenvectors.\n",
        "\n",
        "    Attempts to diagonalize the input square matrix A using eigenvectors.\n",
        "    Returns the diagonal matrix D. You are allowed to use numpy.linalg.inv(P) to compute P^-1.\n",
        "    \"\"\"\n",
        "\n",
        "    D = np.diag(np.linalg.eigvals(A))\n",
        "\n",
        "    P = compute_eigenvectors(A)\n",
        "\n",
        "    P_inv = np.linalg.inv(P)\n",
        "\n",
        "    return D, P, P_inv\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Wx3MYQcFNy"
      },
      "source": [
        "to check your answer use the cells below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KVguFiscFNy"
      },
      "outputs": [],
      "source": [
        "def check_diagonalization(D, A):\n",
        "    is_diagonal = np.allclose(D, np.diag(np.diagonal(D)))\n",
        "    eigenvalues_A = np.linalg.eigvals(A)\n",
        "    diagonal_entries_D = np.diagonal(D)\n",
        "    has_correct_diagonal = np.allclose(np.sort(eigenvalues_A), np.sort(diagonal_entries_D))\n",
        "    has_all_eigenvalues = set(diagonal_entries_D) >= set(eigenvalues_A)\n",
        "\n",
        "    if is_diagonal and has_correct_diagonal and has_all_eigenvalues:\n",
        "        display(HTML('<font color=\"green\">All conditions satisfied:</font> Matrix D properly diagonalizes matrix A.'))\n",
        "    else:\n",
        "        if not is_diagonal:\n",
        "            display(HTML('<font color=\"red\">Error:</font> Matrix D is not diagonal.'))\n",
        "        if not has_correct_diagonal:\n",
        "            display(HTML('<font color=\"red\">Error:</font> Diagonal entries of D are not eigenvalues of A.'))\n",
        "        if not has_all_eigenvalues:\n",
        "            display(HTML('<font color=\"red\">Error:</font> D does not include all eigenvalues of A.'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1sHck8acFNz"
      },
      "outputs": [],
      "source": [
        "A = generate_valid_matrix()\n",
        "\n",
        "D = diagonalize(A)[0]\n",
        "\n",
        "check_diagonalization(D, A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVwOfmw8cFNz"
      },
      "source": [
        "## Properties of Diagonalized Matrix\n",
        "A diagonalized matrix has its eigenvalues on the diagonal. It is easier to compute powers of a diagonal matrix.\n",
        "\n",
        "\n",
        "<h3 style='color: red;'>Question</h3>\n",
        "How do you think we can speed up the process of calculating powers of a matrix by using diagonalization?\n",
        "\n",
        "<h3 style='color: yellow;'>Your answer:</h3>\n",
        "\n",
        "\n",
        "### Diagonalization Speeds Up Matrix Powers\n",
        "\n",
        "The main advantage of diagonalization is that it reduces the problem of calculating powers of a matrix \\( A \\) into a simpler calculation involving scalar powers. Specifically, if \\( A \\) is diagonalizable, then we can write:\n",
        "\\[\n",
        "A = P D P^{-1}\n",
        "\\]\n",
        "where:\n",
        "- \\( D \\) is a diagonal matrix containing the eigenvalues of \\( A \\) along its diagonal,\n",
        "- \\( P \\) is the matrix whose columns are the eigenvectors of \\( A \\).\n",
        "\n",
        "Then, for any integer power \\( k \\), we can compute \\( A^k \\) as:\n",
        "\\[\n",
        "A^k = (P D P^{-1})^k = P D^k P^{-1}.\n",
        "\\]\n",
        "\n",
        "### Simplified Exponentiation with Diagonal Matrices\n",
        "\n",
        "1. **Power of a Diagonal Matrix**: Since \\( D \\) is diagonal, raising it to the \\( k \\)-th power is straightforward. If:\n",
        "   \\[\n",
        "   D = \\begin{bmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{bmatrix},\n",
        "   \\]\n",
        "   then\n",
        "   \\[\n",
        "   D^k = \\begin{bmatrix} \\lambda_1^k & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2^k & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n^k \\end{bmatrix}.\n",
        "   \\]\n",
        "   This is because exponentiating a diagonal matrix involves raising each diagonal element (each eigenvalue \\( \\lambda_i \\)) to the power \\( k \\).\n",
        "\n",
        "2. **Avoiding Repeated Matrix Multiplications**: Normally, calculating \\( A^k \\) by direct multiplication would involve \\( k-1 \\) matrix multiplications, each of which is an \\( O(n^3) \\) operation. However, using diagonalization, we only need to compute:\n",
        "   \\[\n",
        "   A^k = P D^k P^{-1},\n",
        "   \\]\n",
        "   which involves:\n",
        "   - Exponentiating the diagonal matrix \\( D \\) (an \\( O(n) \\) operation),\n",
        "   - Multiplying \\( P D^k P^{-1} \\), which involves only one matrix multiplication step, rather than \\( k-1 \\) multiplications.\n",
        "\n",
        "### Complexity and Efficiency Gains\n",
        "\n",
        "- **Direct Multiplication Complexity**: Computing \\( A^k \\) directly is generally \\( O(n^3) \\times (k-1) \\), which can be very costly when \\( k \\) is large.\n",
        "- **Diagonalization Complexity**: In contrast, with diagonalization, we have \\( O(n) \\) for exponentiating \\( D \\) and \\( O(n^3) \\) for multiplying \\( P \\), \\( D^k \\), and \\( P^{-1} \\), making it much faster for large \\( k \\).\n",
        "\n",
        "\n",
        "## Performance Comparison\n",
        "We will compare the time taken to compute matrix powers using regular methods versus diagonalization.\n",
        "\n",
        "First, we will compute the powers of a matrix using the regular method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrTcUHtkcFN0"
      },
      "outputs": [],
      "source": [
        "def compute_powers(A, n):\n",
        "    result = A\n",
        "    for _ in range(n-1):\n",
        "        result = result @ A\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BstToA3UcFN1"
      },
      "outputs": [],
      "source": [
        "n_values = range(1, 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhip_XUucFN2"
      },
      "outputs": [],
      "source": [
        "regular_times = []\n",
        "\n",
        "for n in n_values:\n",
        "    timer = timeit.Timer(lambda: compute_powers(A, n))\n",
        "    regular_times.append(timer.timeit(number=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C020X530cFN2"
      },
      "source": [
        "Next, we will use your method for calculating power of a matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJiHLcy3cFN2"
      },
      "outputs": [],
      "source": [
        "def compute_powers_your_way(A, n):\n",
        "    \"\"\"\n",
        "    Computes the power of a square matrix A to the nth degree.\n",
        "\n",
        "    Args:\n",
        "    - A (numpy.ndarray): The square matrix for which powers are computed.\n",
        "    - n (int): The exponent to which A is raised.\n",
        "\n",
        "    Returns:\n",
        "    - result (numpy.ndarray): The matrix A raised to the power of n.\n",
        "\n",
        "    \"\"\"\n",
        "    D, P, P_inv = diagonalize(A)\n",
        "\n",
        "    D_n = np.diag(np.diag(D) ** n)\n",
        "\n",
        "    result = P @ D_n @ P_inv\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQQ1DjSBcFN3"
      },
      "outputs": [],
      "source": [
        "diagonal_times = []\n",
        "\n",
        "for n in n_values:\n",
        "    timer = timeit.Timer(lambda: compute_powers_your_way(A, n))\n",
        "    diagonal_times.append(timer.timeit(number=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbMJLLbscFN3"
      },
      "source": [
        "Finally, we will plot the results to compare the performance of both methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKOGfTMYcFN3"
      },
      "outputs": [],
      "source": [
        "plt.plot(n_values, regular_times, label='Regular Method')\n",
        "plt.plot(n_values, diagonal_times, label='Diagonalization Method')\n",
        "plt.xlabel('Exponent (n)')\n",
        "plt.ylabel('Time (s)')\n",
        "plt.legend()\n",
        "plt.title('Performance Comparison')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hV-sahqcFN4"
      },
      "source": [
        "<font color=3C99D size=5>\n",
        "Question Four: KNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUZ_cyQpcFN4"
      },
      "source": [
        "A supervised machine learning approach called k-Nearest Neighbors (KNN) can be applied\n",
        "to classification or regression problems. The approach does not make any assumptions about the underlying distributions of the data since KNN is non-parametric. As opposed to a parametric methodology like linear regression, which calls for us to identify a function that captures the connection between the dependent and independent variables, our method does not need th is. KNN has the benefit of being simple to comprehend. A query point (or test point) is categorized using the k labeled training points that are closest to it when utilized for classification.\n",
        "\n",
        "This Iris dataset includes three iris species with 50 samples each as well as some properties\n",
        "about each flower. One flower species is linearly separable from the other two, but the other\n",
        "two are not linearly separable from each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKlYd-qDcFN4"
      },
      "source": [
        "load the Iris dataset from scikit-learn and convert it into a pandas DataFrame, including the target variable, and display the first few rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgdZ7BKucFN5"
      },
      "outputs": [],
      "source": [
        "#code here\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "df['target'] = pd.Series(data.target)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaqTfRkacFN5"
      },
      "source": [
        "Is the the dataset balanced or not?\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmyExzGBcFN5"
      },
      "outputs": [],
      "source": [
        "df.groupby('target').size().plot(kind='barh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKkvKPelcFN6"
      },
      "source": [
        "Now we need a distance function. In KNN we find the distance between the test data with\n",
        "all our dataset. We can use eucledean or manhattan distance. Here a generalized function is\n",
        "used, with using a parameter p, when p = 1, it’s manhattan distance and when p = 2, it’s\n",
        "eulclidean distance.\n",
        "\n",
        "\n",
        "Write function that calculates the p-norm distance between two lists, a and b, with a default value of p=1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRt00lCgcFN6"
      },
      "outputs": [],
      "source": [
        "def dis(a, b, p=1):\n",
        " if p==1:\n",
        "  dis = sum(abs(val1-val2) for val1, val2 in zip(a,b))\n",
        " elif p==2:\n",
        "  dis = np.sqrt(sum(np.square(val1-val2) for val1, val2 in zip(a,b)))\n",
        " return dis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2yfyc1zcFN6"
      },
      "source": [
        "Now one test point will be executed before splitting data for train and test, it’s to check all\n",
        "implementation, that have been so far. Here the test point that used is test_pt = [4.8, 2.7, 2.5,\n",
        "0.7]. Here after taking the test point the code should measure distance from this data point with\n",
        "all those 150 data points. Then it’ll convert that list to a python DataFrame for further\n",
        "convenience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNj9ttTYcFN7"
      },
      "outputs": [],
      "source": [
        "test_pt = [4.8, 2.7, 2.5, 0.7]\n",
        "df = df.astype('float')\n",
        "dists = df.apply(lambda row : dis([row['sepal length (cm)'],row['sepal width (cm)'],row['petal length (cm)'],row['petal width (cm)']],test_pt), axis=1)\n",
        "dists = pd.DataFrame(dists,index=df.index,columns=['dist'])\n",
        "dists.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jfwPf-ucFN7"
      },
      "source": [
        "Distance DataFrame is sorted to measure which class the nearest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUru-kEmcFN8"
      },
      "outputs": [],
      "source": [
        "def knn_sort(k,dists):\n",
        "    return dists.sort_values(by = 'dist')[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ILM2L2lcFN8"
      },
      "source": [
        "Here k is 5. And taking top k distance the majority of\n",
        "the class will be measure.\n",
        "\n",
        "Find the most common class label among the k nearest neighbors after sorting their distances and counting occurrences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m2modr1cFN8"
      },
      "outputs": [],
      "source": [
        "k=5\n",
        "k_top = knn_sort(k,dists)\n",
        "count_set = [0,0,0]\n",
        "for i in range(k):\n",
        "    count_set[int(df.loc[int(k_top.index[i]),'target'])] += 1\n",
        "count = max(count_set)\n",
        "for i in range(3):\n",
        "    if count_set[i] == count:\n",
        "     print(i)\n",
        "     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A08sPz6CcFN9"
      },
      "source": [
        "Now determine the accurate prediction for that data point and now all the data\n",
        "can be splitted for training and testing.\n",
        "- 75% train, 25% test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCquUrh4cFN9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x = df.drop('target',axis='columns').copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,df['target'],test_size=0.25, random_state=43)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQd8tgTzcFN-"
      },
      "source": [
        "Implements the K-Nearest Neighbors (KNN) algorithm using a custom distance function, with missing parts that need to be filled in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luta05SqcFN-"
      },
      "outputs": [],
      "source": [
        "def KNN(X_train, X_test, y_train, y_test, k, p):\n",
        "    y_predict = []\n",
        "    for test_pt in X_test.itertuples(index=False, name=None):\n",
        "        distances = []\n",
        "        for i in X_train.itertuples(index=False, name=None):\n",
        "            a = dis(i,test_pt,p)\n",
        "            distances.append(a)\n",
        "        dists = pd.DataFrame(data=distances, index=y_train.index,columns=['dist'])\n",
        "        sorted_dists = knn_sort(k,dists)\n",
        "        count_set = {}\n",
        "        for i in sorted_dists.index:\n",
        "            if y_train[i] not in count_set:\n",
        "                count_set[y_train[i]] = 1\n",
        "            else:\n",
        "                count_set[y_train[i]] += 1\n",
        "        most_common = max(count_set, key=count_set.get)\n",
        "        y_predict.append(most_common)\n",
        "\n",
        "    accr = np.sum(y_predict==y_test)/len(y_test)\n",
        "    return accr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6438mbvcFN_"
      },
      "outputs": [],
      "source": [
        "KNN(X_train, X_test, y_train, y_test, 5,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_ai9rk6cFN_"
      },
      "source": [
        "See how the model accuracy varies with respect to k value. A\n",
        "simple loop should be used to call KNN few times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1pUINPrcFN_"
      },
      "outputs": [],
      "source": [
        "accuracies = []\n",
        "best_k = []\n",
        "for i in range(1,100):\n",
        "    accuracies.append(KNN(X_train, X_test, y_train, y_test, i,1))\n",
        "    if accuracies[-1] == max(accuracies):\n",
        "        best_k.append(i)\n",
        "print(best_k)\n",
        "print(max(accuracies))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "ax.plot(range(1,100), accuracies)\n",
        "ax.set_xlabel('# of Nearest Neighbors (k)')\n",
        "ax.set_ylabel('Accuracy (%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGuDraQacFN_"
      },
      "source": [
        "What is the best K value?\n",
        "\n",
        "Answer: for many k values I got accuracy of 97.36 and all k values has been printed in previous output cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHHXStTE3lnZ"
      },
      "source": [
        "<font color=3C99D size=5>\n",
        "Question Five: Estimators of variance\n",
        "\n",
        "In this question, we examine two common estimators of variance. The first estimator is related to the Maximum Likelihood Estimation (MLE):\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
        "$$\n",
        "\n",
        "The next estimator includes Bessel's correction:\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}^2_{\\text{Bessel}} = \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
        "$$\n",
        "\n",
        "Generate samples from a Normal distribution \\(\\mathcal{N}(2, 2)\\) with sample sizes ranging from 2 to 1000, and for each, calculate the bias of both estimators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OaZeMB33lna"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "samples =[]\n",
        "for i in range(2,1000):\n",
        "    sams = np.random.normal(2,2,i)\n",
        "    samples.append(sams)\n",
        "def mle(sample):\n",
        "    avg = np.average(sample)\n",
        "    return(abs(sum(np.square(b - avg) for b in sample)/len(sample)-4))\n",
        "def bessel_c(sample):\n",
        "    avg = np.average(sample)\n",
        "    return(abs(sum(np.square(b - avg) for b in sample)/(len(sample)-1)-4))\n",
        "mle_s=[]\n",
        "bessel_cs=[]\n",
        "for j in range(len(samples)):\n",
        "    mle_s.append(mle(samples[j]))\n",
        "    bessel_cs.append(bessel_c(samples[j]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4AsNAjn3lna"
      },
      "source": [
        "Now, plot the bias values for each of the estimators.\n",
        "\n",
        "Based on these plots, determine whether the two estimators are consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYaB0iZU3lna"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
        "\n",
        "axs[0].plot(mle_s)\n",
        "axs[0].set_title('MLE Plot')\n",
        "axs[0].set_ylabel('Value')\n",
        "axs[1].plot(bessel_cs)\n",
        "axs[1].set_title('Bessel CS Plot')\n",
        "axs[1].set_ylabel('Value')\n",
        "axs[1].set_xlabel('Index')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zinVa6cT3lna"
      },
      "source": [
        "**Answer**: i this example i think both estimators are consistant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFGDFhYN3lna"
      },
      "source": [
        "This time, generate 100,000 samples, each of size 10, from the same Normal distribution $\\mathcal{N}(2, 2)$, and calculate the mean bias for each estimator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6NvERbU3lna"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_samples = 100000\n",
        "sample_size = 10\n",
        "samples = np.random.normal(2, 2, (num_samples, sample_size))\n",
        "mle_s = np.array([mle(sams) for sams in samples])\n",
        "bessel_cs = np.array([bessel_c(sams) for sams in samples])\n",
        "print(np.average(bessel_cs))\n",
        "print(np.average(mle_s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts2pnnx33lna"
      },
      "source": [
        "Based on these values, determine whether the two estimators are unbiased. explain\n",
        "\n",
        "**Answer:Bessel's correction is a crucial adjustment made when estimating population variance from a sample. It ensures that the calculated variance is unbiased by dividing by n−1, reflecting the sample's nature rather than the entire population. This correction helps statisticians obtain a more accurate estimate of population variance from sample data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4aAKsH43lnb"
      },
      "source": [
        "\n",
        "<font color=3C99D size=5>\n",
        "Question Six*: Latent Semantic Analysis (LSA) with Singular Value Decomposition (SVD)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ6-_xkb6zM9"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Latent Semantic Analysis (LSA) is a powerful technique in natural language processing and information retrieval that helps to uncover the underlying structure in a collection of texts. By using Singular Value Decomposition (SVD), LSA transforms high-dimensional textual data into a lower-dimensional space, capturing the most important patterns in the data.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "In this homework, we will explore how LSA can be used to obtain vectors for words and documents, which can then be used for various tasks such as information retrieval, clustering, and semantic analysis. We will achieve this through the following steps:\n",
        "\n",
        "1. **Preparing the Data**: We will start by gathering and preprocessing the text data to construct a term-document matrix.\n",
        "2. **Singular Value Decomposition (SVD)**: We will apply SVD to the term-document matrix to obtain the latent semantic structure.\n",
        "3. **Visualization**: Finally, we will visualize the word and document vectors to understand the relationships between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-E0lxab613y"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIjoq0ZBbQIq"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KTkDSco3lnb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from scipy.linalg import svd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdcbsrKP3lnb"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "In this section, we will prepare our dataset for Latent Semantic Analysis (LSA). This involves loading the dataset, preprocessing the texts, and creating the term-document matrix.\n",
        "\n",
        "### 1. Loading the Dataset\n",
        "\n",
        "First, we will load a dataset of news articles along with their labels indicating whether they are relieble or not. This dataset will serve as the foundation for our analysis.\n",
        "\n",
        "### 2. Preprocessing the Texts\n",
        "\n",
        "To prepare the texts for analysis, we need to perform several preprocessing steps:\n",
        "\n",
        "- **Tokenization**: Breaking down the text into individual words or tokens.\n",
        "- **Removing Stopwords**: Filtering out common words that do not carry significant meaning (e.g., \"and\", \"the\", \"is\").\n",
        "- **Lemmatization**: Reducing words to their base or root form (e.g., \"running\" to \"run\").\n",
        "\n",
        "These steps will help us create a clean and meaningful representation of the text data.\n",
        "\n",
        "### 3. Creating the Term-Document Matrix\n",
        "\n",
        "Once the texts are preprocessed, we will construct a term-document matrix. This matrix will have terms (words) as rows and documents (texts) as columns. Each entry in the matrix will represent the frequency of a term in a document. This matrix will be the input for applying Singular Value Decomposition (SVD) in the next section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pSHrCik3lnb"
      },
      "source": [
        "### Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM5pdEd93lnb"
      },
      "outputs": [],
      "source": [
        "# TODO: Load the dataset\n",
        "df = pd.read_csv('fake_news.csv')\n",
        "# TODO: Show the first 5 rows of the dataset\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXPKouti3lnb"
      },
      "source": [
        "### Preprocessing the Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_svf1kA33lnb"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the input text by performing tokenization, stop words removal, and lemmatization.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text.\n",
        "    \"\"\"\n",
        "    # Initialize the WordNetLemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = text.split()\n",
        "    # Remove stop words and lemmatize the tokens\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    # Join the tokens back into a single string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "    return preprocessed_text\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "# TODO: Apply the preprocess_text function to the 'text' column of the DataFrame\n",
        "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
        "# TODO: Show the first 5 rows of the dataset\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSeP7I--3lnc"
      },
      "source": [
        "### Creating the Term-Document Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIUayJQR3lnc"
      },
      "outputs": [],
      "source": [
        "# TODO: Create the term-document matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# TODO: Create the term-document matrix\n",
        "vectorizer = CountVectorizer()\n",
        "term_document_matrix = vectorizer.fit_transform(df['preprocessed_text'])\n",
        "term_document_df = pd.DataFrame(term_document_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(term_document_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IervEFpe3lnc"
      },
      "source": [
        "## Applying Singular Value Decomposition (SVD)\n",
        "\n",
        "In this section, we will apply Singular Value Decomposition (SVD) to the term-document matrix to extract the latent semantic structure of the data. The right singular vectors obtained from SVD will be used as the embeddings for our documents.\n",
        "\n",
        "### Singular Value Decomposition (SVD)\n",
        "\n",
        "Singular Value Decomposition is a matrix factorization technique that decomposes a matrix $A$ into three matrices:\n",
        "\n",
        "$$\n",
        "A = U \\Sigma V^T\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $U$ is an orthogonal matrix containing the left singular vectors.\n",
        "- $\\Sigma$ is a diagonal matrix containing the singular values.\n",
        "- $V^T$ is the transpose of an orthogonal matrix containing the right singular vectors.\n",
        "\n",
        "In the context of text data, the term-document matrix $A$ can be decomposed using SVD to reveal the underlying semantic structure.\n",
        "\n",
        "### Extracting Document Embeddings\n",
        "\n",
        "The right singular vectors obtained from the SVD are particularly useful for our analysis. These vectors, which are the columns of the matrix $V$, represent the documents in a reduced-dimensional semantic space. These document embeddings capture the essential features of the texts and can be used for various downstream tasks such as clustering, classification, and visualization.\n",
        "\n",
        "By applying SVD to the term-document matrix, we will obtain a set of document embeddings that can be visualized and analyzed to gain insights into the relationships between different documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXN2miC53lnc"
      },
      "outputs": [],
      "source": [
        "# TODO: Perform Singular Value Decomposition (SVD) on the term-document matrix (this might take a few minutes)\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "n_components = 30\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "document_embeddings = svd.fit_transform(term_document_matrix)\n",
        "document_embeddings_df = pd.DataFrame(document_embeddings)\n",
        "print(document_embeddings_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0cidq03lnc"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "In this section, we will visualize the document embeddings obtained from Singular Value Decomposition (SVD). By plotting the first two singular vectors, we can create a 2D scatter plot of the documents. Each document will be colored based on its respective label (reliable or unreliable) to help us visually analyze the separation between different classes.\n",
        "\n",
        "### 2D Embeddings\n",
        "\n",
        "To visualize the document embeddings, we will use the first two right singular vectors obtained from SVD. These vectors correspond to the two largest singular values and capture the most significant features of the data.\n",
        "\n",
        "### Scatter Plot\n",
        "\n",
        "We will create a scatter plot where:\n",
        "- Each point represents a document.\n",
        "- The x-coordinate of a point corresponds to its first singular vector value.\n",
        "- The y-coordinate of a point corresponds to its second singular vector value.\n",
        "- The color of each point indicates whether the document is labeled as fake news or true news.\n",
        "\n",
        "This visualization will allow us to see how well the SVD has separated the different classes of documents in a 2D space. It can also provide insights into the structure and distribution of the documents based on their latent semantic features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGpzzj_F3lnc"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot the terms in the 2D space\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(document_embeddings[:, 0], document_embeddings[:, 1],\n",
        "            c=df['label'], cmap='viridis', alpha=0.5)\n",
        "plt.xlabel(\"First Component\")\n",
        "plt.ylabel(\"Second Component\")\n",
        "plt.title(\"2D Scatter Plot of Document Embeddings Based on SVD\")\n",
        "plt.colorbar(label='Label (0 = Not Fake, 1 = Fake)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPNYi-e3lnc"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this homework, we explored the use of Latent Semantic Analysis (LSA) with Singular Value Decomposition (SVD) to obtain word and document vectors from a dataset of texts labeled as fake news or true news. We went through the steps of loading and preprocessing the data, applying SVD, and visualizing the document embeddings in a 2D space.\n",
        "\n",
        "### Answer these questions:\n",
        "\n",
        "1. **Why do you think reliable and unreliable news separated without using sample labels?**\n",
        "**Reliable and unreliable news may separate without using sample labels because LSA, through SVD, identifies latent structures and semantic patterns in the text data itself. Even without explicit labels, the mathematical properties of the text—such as the frequency of certain words, phrases, and their contextual relationships—can reveal inherent differences in topics, tone, and language style between reliable and unreliable sources.**\n",
        "**For instance, reliable news articles might tend to use more formal language, specific terminologies, and fewer sensationalist words compared to unreliable articles, which may be characterized by hyperbolic language, emotional appeals, or specific phrases often associated with misinformation. The SVD captures these variations in a lower-dimensional space, enabling separation based on these latent features rather than direct labeling.**\n",
        "2. **What do the left singular vectors mean?**\n",
        "**The left singular vectors obtained from SVD represent the relationships of the terms (or words) in the context of the documents. Each left singular vector corresponds to a term's contribution to the latent semantic structure captured by the SVD. In practical terms:**\n",
        "**- Each left singular vector can be seen as a \"feature\" of the documents represented in the term-document matrix. They help define how terms are distributed across the different documents in the corpus.**\n",
        "**- The magnitude of each vector component indicates how strongly a term is associated with a particular latent semantic structure, while the direction of the vector can represent the relationship of that term to other terms in the context of the documents.**\n",
        "**- By examining these vectors, we can gain insights into which terms are most significant for distinguishing between different topics or categories (like fake vs. true news) within the text.**\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
